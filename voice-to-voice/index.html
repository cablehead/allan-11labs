<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ElevenLabs Conversational AI</title>
  <style>
    body { font-family: system-ui; margin: 2rem; max-width: 600px; }
    input, select, button { width: 100%; padding: 0.5rem; margin: 0.5rem 0; }
    .flex { display: flex; gap: 0.5rem; }
    .flex input { flex: 1; }
    .flex button { width: auto; padding: 0.5rem 1rem; }
    #log { font-family: monospace; background: #f5f5f5; padding: 0.5rem; height: 300px; overflow-y: auto; }
    button { background: #007bff; color: white; border: none; cursor: pointer; }
    button:disabled { background: #ccc; }
    button:active { background: #0056b3; }
    #push { 
      background: #28a745; 
      font-size: 1.2em; 
      padding: 1rem; 
      margin: 1rem 0; 
      user-select: none;
    }
    #push.active { background: #dc3545; }
    #push:hover { background: #218838; }
    #push.active:hover { background: #c82333; }
    #hangup { background: #dc3545; }
    #hangup:hover { background: #c82333; }
    .user-msg { color: #0066cc; font-weight: bold; }
    .agent-msg { color: #cc6600; }
    .system-msg { color: #666; }
    .api-section { border: 1px solid #ddd; padding: 1rem; margin: 1rem 0; border-radius: 4px; }
    .api-section h3 { margin-top: 0; }
  </style>
</head>
<body>
  <h1>ElevenLabs Conversational AI</h1>

  <div class="api-section">
    <h3>ElevenLabs Configuration</h3>
    <label>API Key:</label>
    <div class="flex">
      <input type="password" id="apiKey" placeholder="Enter ElevenLabs API key">
      <button id="clear">Clear</button>
    </div>
    
    <label>Agent ID:</label>
    <input type="text" id="agentId" placeholder="Enter Agent ID">
    
    <label>Voice:</label>
    <select id="voice" disabled><option>Enter API key first...</option></select>
  </div>

  <div class="api-section">
    <h3>Claude Configuration</h3>
    <label>Anthropic API Key:</label>
    <div class="flex">
      <input type="password" id="haikuKey" placeholder="Enter Anthropic API key">
      <button id="clearHaiku">Clear</button>
    </div>
  </div>

  <button id="push" disabled>üé§ Start Microphone</button>
  <button id="hangup" disabled>üìû Hang Up</button>

  <div id="log"></div>

  <script>
    const $ = id => document.getElementById(id);
    const apiKey = $('apiKey'), voice = $('voice'), agentId = $('agentId');
    const haikuKey = $('haikuKey'), push = $('push'), hangup = $('hangup'), log = $('log');
    
    // globals
    let ws, audioCtx, worklet, conversationId, stream;
    const history = [];
    
    const HAIKU_HEADERS = {
      "Content-Type": "application/json",
      "anthropic-version": "2023-06-01",
      "Accept": "text/event-stream",
      "anthropic-dangerous-direct-browser-access": "true"
    };
    const HAIKU_MODEL = "claude-3-5-haiku-latest";

    function addLog(msg, className = 'system-msg') {
      const lines = log.innerHTML.split('<br>').filter(Boolean);
      const timestamp = `[${new Date().toLocaleTimeString()}]`;
      let msgClass = className;
      
      // Determine message type and styling
      if (msg.startsWith('USER:')) {
        msgClass = 'user-msg';
      } else if (msg.startsWith('AGENT>')) {
        msgClass = 'agent-msg';
      }
      
      lines.push(`<span class="${msgClass}">${timestamp} ${msg}</span>`);
      if (lines.length > 100) lines.splice(0, lines.length - 100);
      log.innerHTML = lines.join('<br>');
      log.scrollTop = log.scrollHeight;
    }

    function updateButtons() {
      const hasElevenLabs = apiKey.value && voice.value && agentId.value;
      const hasHaiku = haikuKey.value;
      const isConnected = ws && ws.readyState === WebSocket.OPEN;
      
      push.disabled = !(hasElevenLabs && hasHaiku);
      hangup.disabled = !isConnected;
    }

    async function loadVoices() {
      try {
        voice.innerHTML = '<option>Loading...</option>';
        const res = await fetch('https://api.elevenlabs.io/v1/voices', {
          headers: { 'xi-api-key': apiKey.value }
        });
        const data = await res.json();
        
        voice.innerHTML = '<option value="">Select voice...</option>';
        data.voices.forEach(v => {
          const opt = document.createElement('option');
          opt.value = v.voice_id;
          opt.textContent = v.name;
          voice.appendChild(opt);
        });
        voice.disabled = false;
        addLog(`‚úì Loaded ${data.voices.length} voices`);
      } catch (e) {
        voice.innerHTML = '<option>Error loading</option>';
        addLog(`‚úó Error loading voices: ${e.message}`);
      }
      updateButtons();
    }

    // WebSocket bootstrap
    function openSocket(agentIdValue, apiKeyValue, voiceIdValue) {
      if (ws && ws.readyState === WebSocket.OPEN) return;
      
      const url = `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${agentIdValue}`;
      ws = new WebSocket(url);
      ws.binaryType = "arraybuffer";
      
      ws.onopen = async () => {
        addLog("‚úì WebSocket connected");
        audioCtx = new (AudioContext || webkitAudioContext)({ sampleRate: 16000 });
        
        // Ensure AudioContext is running (required in some browsers)
        if (audioCtx.state === 'suspended') {
          try {
            await audioCtx.resume();
            addLog(`üîÑ AudioContext resumed`);
          } catch (e) {
            addLog(`‚ö†Ô∏è AudioContext resume failed: ${e.message}`);
          }
        }
        
        addLog(`üéß AudioContext created (sampleRate: ${audioCtx.sampleRate}Hz, state: ${audioCtx.state})`);
        sendInitPacket(apiKeyValue, voiceIdValue);
        updateButtons();
      };
      
      ws.onmessage = handleServerEvent;
      ws.onclose = cleanupUI;
      ws.onerror = () => addLog('‚ùå WebSocket connection error');
    }

    function sendInitPacket(key, voiceId) {
      const init = {
        type: "conversation_initiation_client_data",
        conversation_initiation_client_data: {
          client_supplied_trace_id: crypto.randomUUID(),
          agent: { voice_id: voiceId },
          llm: { provider: "custom" },
          text_to_speech: { model_id: "eleven_flash_v2_5" }
        }
      };
      ws.send(JSON.stringify(init));
      addLog("ü™Ñ Session started");
    }

    // Server ‚Üí client events
    async function handleServerEvent({ data }) {
      const msg = JSON.parse(data);
      
      // Debug logging for all events
      if (msg.type !== "ping" && msg.type !== "vad_score") {
        console.debug("üì® Received:", msg.type, msg);
      }

      switch (msg.type) {
        case "ping":
          ws.send(JSON.stringify({ type: "pong", event_id: msg.ping_event.event_id }));
          break;

        case "vad_score":
          // Show VAD activity occasionally 
          if (msg.vad_score_event.vad_score > 0.6 && (!window._lastVadLog || Date.now() - window._lastVadLog > 2000)) {
            addLog(`üéØ Voice activity detected (${msg.vad_score_event.vad_score.toFixed(2)})`);
            window._lastVadLog = Date.now();
          }
          break;

        case "user_transcript":
          addLog("USER: " + msg.user_transcript.text);
          streamToBrain(msg.user_transcript.text);
          break;

        case "audio":
          playAudio64(msg.audio_event.audio_base_64);
          break;

        case "conversation_initiation_client_data_response":
          addLog("‚úì Conversation initiated");
          break;

        case "conversation_initiation_metadata":
          conversationId = msg.conversation_initiation_metadata_event.conversation_id;
          addLog(`‚úì Conversation metadata received (ID: ${conversationId.slice(-8)})`);
          break;

        default:
          console.warn("unknown event", msg.type, msg);
          addLog(`‚ùì Unknown event: ${msg.type}`);
      }
    }

    // SSE iterator for Anthropic streaming
    async function* sseIterator(stream) {
      const reader = stream.getReader();
      const decoder = new TextDecoder("utf-8");
      let buffer = "";
      
      while (true) {
        const { value, done } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });

        let sep;
        while ((sep = buffer.indexOf("\n\n")) !== -1) {
          const block = buffer.slice(0, sep).trim();
          buffer = buffer.slice(sep + 2);

          const [, json] = block.match(/^data:\s*(.*)$/m) || [];
          if (!json) continue;
          yield JSON.parse(json);
        }
      }
    }

    // Bridge to Claude Haiku
    async function streamToBrain(userText) {
      history.push({ role: "user", content: userText });

      const payload = {
        model: HAIKU_MODEL,
        stream: true,
        max_tokens: 512,
        messages: history
      };

      try {
        const resp = await fetch("https://api.anthropic.com/v1/messages", {
          method: "POST",
          headers: {
            ...HAIKU_HEADERS,
            "x-api-key": haikuKey.value
          },
          body: JSON.stringify(payload)
        });

        let buffer = "";
        let fullResponse = "";
        
        for await (const evt of sseIterator(resp.body)) {
          if (evt.type === "content_block_delta" && evt.delta.type === "text_delta") {
            buffer += evt.delta.text;
            fullResponse += evt.delta.text;

            // Send after sentence or ~40 tokens
            if (buffer.split(/\s+/).length > 40 || /[.!?]\s$/.test(buffer)) {
              sendAgentResponse(buffer, false);
              buffer = "";
            }
          }

          if (evt.type === "message_stop") {
            if (buffer) sendAgentResponse(buffer, true);
            history.push({ role: "assistant", content: fullResponse });
          }
        }
      } catch (e) {
        addLog(`‚ùå Brain error: ${e.message}`);
        sendAgentResponse("I'm sorry, I encountered an error processing your request.", true);
      }
    }

    function sendAgentResponse(text, isFinal) {
      ws.send(JSON.stringify({
        type: "agent_response",
        agent_response_event: {
          agent_response: text,
          is_final: isFinal ?? false
        }
      }));
      addLog("AGENT> " + text);
    }

    // Microphone capture
    async function startCapture() {
      try {
        if (!audioCtx) {
          addLog("‚ùå AudioContext not ready");
          return;
        }
        
        // Ensure AudioContext is running
        if (audioCtx.state === 'suspended') {
          try {
            await audioCtx.resume();
            addLog(`üîÑ AudioContext resumed for capture`);
          } catch (e) {
            addLog(`‚ùå AudioContext resume failed: ${e.message}`);
            return;
          }
        }
        
        addLog(`üéß AudioContext state: ${audioCtx.state}`);
        
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // Debug: check stream properties
        const audioTracks = stream.getAudioTracks();
        addLog(`üé§ Got ${audioTracks.length} audio track(s)`);
        audioTracks.forEach((track, i) => {
          console.debug(`üìä Track ${i}:`, {
            enabled: track.enabled,
            muted: track.muted,
            readyState: track.readyState,
            label: track.label
          });
          addLog(`Track ${i}: ${track.label} (${track.readyState})`);
        });
        
        const source = audioCtx.createMediaStreamSource(stream);
        addLog(`üîä Audio source created`);
        
        // Load worklet with better error handling
        try {
          await audioCtx.audioWorklet.addModule("pcm-worklet.js");
          addLog("‚úì Audio worklet loaded");
        } catch (e) {
          addLog(`‚ùå Failed to load audio worklet: ${e.message}`);
          throw e;
        }
        
        worklet = new AudioWorkletNode(audioCtx, "pcm-encoder");
        addLog(`üíµ AudioWorkletNode created`);
        
        // Test if worklet is working at all
        setTimeout(() => {
          console.debug("üìã Worklet status check - if no audio frames logged, there's a connection issue");
        }, 2000);
        
        worklet.port.onmessage = ({ data }) => {
          // Debug: log every message we get from worklet
          if (!window._audioDebugCount) window._audioDebugCount = 0;
          window._audioDebugCount++;
          
          if (window._audioDebugCount <= 5 || window._audioDebugCount % 50 === 0) {
            console.debug(`üü¢ Worklet message #${window._audioDebugCount}:`, typeof data, data?.length || data?.byteLength || "unknown size");
          }
          
          if (!ws || ws.readyState !== WebSocket.OPEN) {
            console.debug("‚ùå WebSocket not ready, dropping audio chunk");
            return;
          }

          // --------------------------------------------------
          // 1. Ensure we hand the server a BASE-64 string
          // --------------------------------------------------
          let b64;
          if (typeof data === "string") {
            // old worklet already did the btoa()
            b64 = data;
          } else {
            // data is an ArrayBuffer -> Uint8Array -> binary string -> btoa
            const view = new Uint8Array(data);
            let bin = "";
            for (let i = 0; i < view.length; i++) bin += String.fromCharCode(view[i]);
            b64 = btoa(bin);
          }

          // --------------------------------------------------
          // 2. Send the chunk
          // --------------------------------------------------
          ws.send(
            JSON.stringify({
              type: "user_audio_chunk",
              user_audio_chunk: { audio: b64, timestamp: Date.now() }
            })
          );

          // --------------------------------------------------
          // 3. Debug: show one packet / s
          // --------------------------------------------------
          if (!window._lastLog || Date.now() - window._lastLog > 1000) {
            console.debug("üéß sent audio chunk", b64.length, "bytes (base-64)");
            addLog(`üéß Sending audio chunks (${b64.length} bytes)`); 
            window._lastLog = Date.now();
          }
        };
        
        try {
          source.connect(worklet);
          addLog(`üîó Audio routing connected: Mic ‚Üí Worklet`);
          
          // Test connection by checking if we get any audio data
          setTimeout(() => {
            if (!window._audioDebugCount || window._audioDebugCount === 0) {
              addLog("‚ö†Ô∏è WARNING: No audio data received after 3 seconds - check microphone permissions!");
            }
          }, 3000);
          
        } catch (e) {
          addLog(`‚ùå Audio connection failed: ${e.message}`);
          throw e;
        }
        addLog("üé§ Microphone active");
      } catch (e) {
        addLog(`‚ùå Microphone error: ${e.message}`);
      }
    }

    // Audio playback
    function playAudio64(b64) {
      try {
        const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));
        audioCtx.decodeAudioData(bytes.buffer).then(buf => {
          const src = audioCtx.createBufferSource();
          src.buffer = buf;
          src.connect(audioCtx.destination);
          src.start();
        });
      } catch (e) {
        addLog(`‚ùå Audio playback error: ${e.message}`);
      }
    }

    function cleanupUI() {
      if (worklet) {
        worklet.disconnect();
        worklet = null;
      }
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
        stream = null;
      }
      
      // Reset microphone button state
      micActive = false;
      push.classList.remove('active');
      push.textContent = "üé§ Start Microphone";
      
      updateButtons();
      addLog("üîå Connection closed");
    }

    // Event listeners
    apiKey.oninput = () => {
      if (apiKey.value.length > 20) {
        localStorage.elevenlabs_key = apiKey.value;
        addLog(`üîë ElevenLabs API key entered`);
        loadVoices();
      } else {
        voice.innerHTML = '<option>Enter API key first...</option>';
        voice.disabled = true;
        updateButtons();
      }
    };

    haikuKey.oninput = () => {
      if (haikuKey.value.length > 20) {
        localStorage.haiku_key = haikuKey.value;
        addLog(`üîë Anthropic API key entered`);
        updateButtons();
      } else {
        updateButtons();
      }
    };

    voice.onchange = agentId.oninput = updateButtons;

    $('clear').onclick = () => {
      delete localStorage.elevenlabs_key;
      apiKey.value = '';
      voice.innerHTML = '<option>Enter API key first...</option>';
      voice.disabled = true;
      addLog('üóëÔ∏è ElevenLabs API key cleared');
      updateButtons();
    };

    $('clearHaiku').onclick = () => {
      delete localStorage.haiku_key;
      haikuKey.value = '';
      addLog('üóëÔ∏è Anthropic API key cleared');
      updateButtons();
    };

    // Toggle microphone on/off
    let micActive = false;
    
    push.onclick = async () => {
      if (!micActive) {
        // Start microphone
        if (!ws || ws.readyState !== WebSocket.OPEN) {
          addLog("üîó Connecting to ElevenLabs...");
          openSocket(agentId.value, apiKey.value, voice.value);
          
          // Wait for connection to establish
          const maxWait = 5000; // 5 seconds max
          const startTime = Date.now();
          
          while ((!ws || ws.readyState !== WebSocket.OPEN) && (Date.now() - startTime < maxWait)) {
            await new Promise(resolve => setTimeout(resolve, 100));
          }
          
          if (!ws || ws.readyState !== WebSocket.OPEN) {
            addLog("‚ùå Connection timeout - try again");
            return;
          }
        }
        
        addLog("üé§ Starting microphone...");
        await startCapture();
        
        micActive = true;
        push.classList.add('active');
        push.textContent = "üî¥ Stop Microphone";
        
      } else {
        // Stop microphone
        if (worklet) {
          worklet.disconnect();
          worklet = null;
        }
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
          stream = null;
        }
        
        micActive = false;
        push.classList.remove('active');
        push.textContent = "üé§ Start Microphone";
        addLog("üé§ Microphone stopped");
      }
    };

    hangup.onclick = () => {
      if (ws) {
        ws.close();
        ws = null;
      }
      cleanupUI();
      history.length = 0; // Clear conversation history
      addLog("üìû Call ended");
    };

    // Load saved keys
    if (localStorage.elevenlabs_key) {
      apiKey.value = localStorage.elevenlabs_key;
      addLog(`üìÅ Loaded saved ElevenLabs API key`);
      loadVoices();
    }

    if (localStorage.haiku_key) {
      haikuKey.value = localStorage.haiku_key;
      addLog(`üìÅ Loaded saved Anthropic API key`);
    }

    updateButtons();
  </script>
</body>
</html>