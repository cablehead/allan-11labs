<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Audio Capture Test</title>
  <style>
    body { 
      font-family: system-ui; 
      margin: 2rem; 
      max-width: 700px; 
      line-height: 1.5;
    }
    button { 
      width: 100%; 
      padding: 1rem; 
      margin: 0.5rem 0; 
      background: #007bff; 
      color: white; 
      border: none; 
      cursor: pointer; 
      font-size: 1rem;
      border-radius: 4px;
    }
    button:disabled { 
      background: #ccc; 
      cursor: not-allowed;
    }
    button.active {
      background: #dc3545;
    }
    button:hover:not(:disabled) {
      background: #0056b3;
    }
    button.active:hover {
      background: #c82333;
    }
    #log { 
      font-family: monospace; 
      background: #f5f5f5; 
      padding: 1rem; 
      height: 300px; 
      overflow-y: auto; 
      margin: 1rem 0;
      border: 1px solid #ddd;
      border-radius: 4px;
      white-space: pre-wrap;
    }
    .info {
      background: #e3f2fd;
      padding: 1rem;
      border-radius: 4px;
      margin: 1rem 0;
    }
    .device-info {
      font-size: 0.9em;
      color: #666;
      background: #f8f9fa;
      padding: 0.5rem;
      border-radius: 4px;
      margin: 1rem 0;
    }
    .audio-stats {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      gap: 1rem;
      margin: 1rem 0;
    }
    .stat {
      background: #f8f9fa;
      padding: 0.5rem;
      border-radius: 4px;
      text-align: center;
    }
    .stat-value {
      font-size: 1.2em;
      font-weight: bold;
      color: #007bff;
    }
    .stat-label {
      font-size: 0.8em;
      color: #666;
    }
  </style>
</head>
<body>
  <nav style="margin-bottom: 1rem;">
    <a href="../" style="color: #007bff; text-decoration: none; font-size: 0.9rem;">&larr; Comedy Colliders</a>
  </nav>
  <h1>Audio Capture Test</h1>
  
  <div class="info">
    <strong>Purpose:</strong> This test isolates the audio capture mechanism from voice-to-voice:
    <br>‚Ä¢ Request microphone access ‚Üí getUserMedia ‚Üí AudioContext ‚Üí AudioWorklet
    <br>‚Ä¢ Process audio chunks ‚Üí convert to PCM ‚Üí capture to buffer
    <br>‚Ä¢ Test without WebSocket/API dependencies
  </div>

  <div class="device-info" id="deviceInfo">
    Loading device information...
  </div>

  <div class="audio-stats">
    <div class="stat">
      <div class="stat-value" id="chunksReceived">0</div>
      <div class="stat-label">Audio Chunks</div>
    </div>
    <div class="stat">
      <div class="stat-value" id="totalBytes">0</div>
      <div class="stat-label">Bytes Captured</div>
    </div>
    <div class="stat">
      <div class="stat-value" id="sampleRate">-</div>
      <div class="stat-label">Sample Rate</div>
    </div>
    <div class="stat">
      <div class="stat-value" id="audioLevel">0%</div>
      <div class="stat-label">Audio Level</div>
    </div>
  </div>

  <div id="log"></div>

  <button id="startCapture">üé§ Start Audio Capture</button>
  <button id="stopCapture" disabled>‚èπÔ∏è Stop Capture</button>
  <button id="playback" disabled>‚ñ∂Ô∏è Play Captured Audio</button>

  <script>
    const $ = id => document.getElementById(id);
    const log = $('log');
    let audioCtx = null;
    let stream = null;
    let worklet = null;
    let isCapturing = false;
    let audioBuffer = [];
    let stats = {
      chunks: 0,
      bytes: 0,
      lastLevel: 0
    };

    function addLog(msg) {
      const lines = log.textContent.split('\n').filter(Boolean);
      const timestamp = new Date().toLocaleTimeString();
      lines.push(`[${timestamp}] ${msg}`);
      if (lines.length > 100) lines.splice(0, lines.length - 100);
      log.textContent = lines.join('\n');
      log.scrollTop = log.scrollHeight;
    }

    function updateStats() {
      $('chunksReceived').textContent = stats.chunks;
      $('totalBytes').textContent = (stats.bytes / 1024).toFixed(1) + 'KB';
      $('sampleRate').textContent = audioCtx ? audioCtx.sampleRate + 'Hz' : '-';
      $('audioLevel').textContent = Math.round(stats.lastLevel * 100) + '%';
    }

    function detectDevice() {
      const ua = navigator.userAgent;
      const isIOS = /iPad|iPhone|iPod/.test(ua);
      const isMacOS = /Mac OS X/.test(ua) && !isIOS;
      const isSafari = /Safari/.test(ua) && !/Chrome/.test(ua);
      
      let info = `Device: ${isIOS ? 'iOS' : isMacOS ? 'macOS' : 'Other'}\n`;
      info += `Browser: ${isSafari ? 'Safari' : 'Chrome/Other'}\n`;
      info += `getUserMedia: ${navigator.mediaDevices?.getUserMedia ? '‚úì' : '‚úó'}\n`;
      info += `AudioContext: ${window.AudioContext ? '‚úì' : '‚úó'} webkitAudioContext: ${window.webkitAudioContext ? '‚úì' : '‚úó'}\n`;
      info += `AudioWorklet: ${window.AudioContext?.prototype.audioWorklet ? '‚úì' : '‚úó'}`;
      
      $('deviceInfo').textContent = info;
      addLog(`Device: ${isIOS ? 'iOS' : isMacOS ? 'macOS' : 'Other'}`);
    }

    async function startAudioCapture() {
      try {
        addLog('üöÄ Starting audio capture...');
        
        // Create AudioContext
        audioCtx = new (AudioContext || webkitAudioContext)({ sampleRate: 16000 });
        addLog(`üéß AudioContext created (sampleRate: ${audioCtx.sampleRate}Hz, state: ${audioCtx.state})`);
        
        // Resume if suspended (iOS requirement)
        if (audioCtx.state === 'suspended') {
          await audioCtx.resume();
          addLog(`üîÑ AudioContext resumed, new state: ${audioCtx.state}`);
        }
        
        // Request microphone access
        addLog('üé§ Requesting microphone access...');
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });
        
        // Debug stream info
        const audioTracks = stream.getAudioTracks();
        addLog(`‚úì Got ${audioTracks.length} audio track(s)`);
        
        audioTracks.forEach((track, i) => {
          const settings = track.getSettings();
          addLog(`Track ${i}: ${track.label} (${track.readyState})`);
          addLog(`  Settings: ${settings.sampleRate}Hz, ${settings.channelCount}ch`);
        });
        
        // Create media stream source
        const source = audioCtx.createMediaStreamSource(stream);
        addLog('üîä Audio source created');
        
        // Load audio worklet
        try {
          await audioCtx.audioWorklet.addModule('pcm-worklet.js');
          addLog('‚úì Audio worklet loaded');
        } catch (e) {
          addLog(`‚ùå Failed to load audio worklet: ${e.message}`);
          throw e;
        }
        
        // Create worklet node
        worklet = new AudioWorkletNode(audioCtx, 'pcm-encoder');
        addLog('üíæ AudioWorkletNode created');
        
        // Handle audio data from worklet
        worklet.port.onmessage = ({ data }) => {
          stats.chunks++;
          stats.bytes += data.byteLength;
          
          // Calculate audio level (RMS)
          const view = new Int16Array(data);
          let sum = 0;
          for (let i = 0; i < view.length; i++) {
            sum += view[i] * view[i];
          }
          stats.lastLevel = Math.sqrt(sum / view.length) / 32767;
          
          // Store audio data for playback test
          audioBuffer.push(new Uint8Array(data));
          
          // Update stats display
          updateStats();
          
          // Log first few chunks and then periodically
          if (stats.chunks <= 5 || stats.chunks % 50 === 0) {
            addLog(`üìä Audio chunk #${stats.chunks}: ${data.byteLength} bytes, level: ${Math.round(stats.lastLevel * 100)}%`);
          }
        };
        
        // Connect audio pipeline
        source.connect(worklet);
        addLog('üîó Audio pipeline connected: Mic ‚Üí Worklet');
        
        // Test connection after a delay
        setTimeout(() => {
          if (stats.chunks === 0) {
            addLog('‚ö†Ô∏è WARNING: No audio data received - check microphone permissions!');
          } else {
            addLog(`‚úÖ Audio capture working! Received ${stats.chunks} chunks`);
          }
        }, 3000);
        
        isCapturing = true;
        $('startCapture').disabled = true;
        $('stopCapture').disabled = false;
        addLog('üé§ Audio capture active');
        
      } catch (error) {
        addLog(`‚ùå Audio capture failed: ${error.message}`);
        console.error('Audio capture error:', error);
        cleanup();
      }
    }

    function stopAudioCapture() {
      addLog('‚èπÔ∏è Stopping audio capture...');
      cleanup();
      $('playback').disabled = audioBuffer.length === 0;
      addLog(`‚úì Capture stopped. Captured ${stats.chunks} chunks (${(stats.bytes / 1024).toFixed(1)}KB)`);
    }

    async function playbackCapturedAudio() {
      if (audioBuffer.length === 0) {
        addLog('‚ùå No audio data to play back');
        return;
      }
      
      try {
        addLog('‚ñ∂Ô∏è Playing back captured audio...');
        
        if (!audioCtx) {
          audioCtx = new (AudioContext || webkitAudioContext)({ sampleRate: 16000 });
        }
        
        if (audioCtx.state === 'suspended') {
          await audioCtx.resume();
        }
        
        // Combine all audio chunks into one buffer
        const totalBytes = audioBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
        const combined = new Uint8Array(totalBytes);
        let offset = 0;
        
        for (const chunk of audioBuffer) {
          combined.set(chunk, offset);
          offset += chunk.length;
        }
        
        addLog(`üì¶ Combined ${audioBuffer.length} chunks into ${totalBytes} bytes`);
        
        // Convert to AudioBuffer for playback
        const int16View = new Int16Array(combined.buffer);
        const audioBufferNode = audioCtx.createBuffer(1, int16View.length, audioCtx.sampleRate);
        const channelData = audioBufferNode.getChannelData(0);
        
        // Convert int16 to float32
        for (let i = 0; i < int16View.length; i++) {
          channelData[i] = int16View[i] / 32767;
        }
        
        // Play the audio
        const source = audioCtx.createBufferSource();
        source.buffer = audioBufferNode;
        source.connect(audioCtx.destination);
        source.start();
        
        addLog(`üîä Playing ${audioBufferNode.duration.toFixed(2)} seconds of captured audio`);
        
        source.onended = () => {
          addLog('‚úÖ Playback completed');
        };
        
      } catch (error) {
        addLog(`‚ùå Playback failed: ${error.message}`);
        console.error('Playback error:', error);
      }
    }

    function cleanup() {
      if (worklet) {
        worklet.disconnect();
        worklet = null;
      }
      
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
        stream = null;
      }
      
      isCapturing = false;
      $('startCapture').disabled = false;
      $('stopCapture').disabled = true;
    }

    // Event listeners
    $('startCapture').onclick = startAudioCapture;
    $('stopCapture').onclick = stopAudioCapture;
    $('playback').onclick = playbackCapturedAudio;

    // Initialize
    document.addEventListener('DOMContentLoaded', () => {
      addLog('Audio Capture Test initialized');
      detectDevice();
      updateStats();
      
      // iOS audio unlock
      document.addEventListener('touchstart', () => {
        addLog('üì± Touch detected - audio unlocked for iOS');
      }, { once: true });
    });

    // Cleanup on page unload
    window.addEventListener('beforeunload', cleanup);
  </script>
</body>
</html>