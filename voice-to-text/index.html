<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice-to-Text Test</title>
  <style>
    body { 
      font-family: system-ui; 
      margin: 2rem; 
      max-width: 700px; 
      line-height: 1.5;
    }
    input, select, button { 
      width: 100%; 
      padding: 0.5rem; 
      margin: 0.5rem 0; 
      border: 1px solid #ddd;
      border-radius: 4px;
    }
    .flex { 
      display: flex; 
      gap: 0.5rem; 
    }
    .flex input { 
      flex: 1; 
    }
    .flex button { 
      width: auto; 
      padding: 0.5rem 1rem; 
    }
    button { 
      background: #007bff; 
      color: white; 
      border: none; 
      cursor: pointer; 
      font-size: 1rem;
    }
    button:disabled { 
      background: #ccc; 
      cursor: not-allowed;
    }
    button:hover:not(:disabled) {
      background: #0056b3;
    }
    #micButton { 
      background: #28a745; 
      font-size: 1.2em; 
      padding: 1rem; 
      margin: 1rem 0; 
      user-select: none;
    }
    #micButton.active { 
      background: #dc3545; 
    }
    #micButton:hover:not(:disabled) { 
      background: #218838; 
    }
    #micButton.active:hover { 
      background: #c82333; 
    }
    #log { 
      font-family: monospace; 
      background: #f5f5f5; 
      padding: 1rem; 
      height: 300px; 
      overflow-y: auto; 
      margin: 1rem 0;
      border: 1px solid #ddd;
      border-radius: 4px;
      white-space: pre-wrap;
    }
    .info {
      background: #e3f2fd;
      padding: 1rem;
      border-radius: 4px;
      margin: 1rem 0;
    }
    .api-section {
      border: 1px solid #ddd;
      padding: 1rem;
      margin: 1rem 0;
      border-radius: 4px;
      background: #fafafa;
    }
    .api-section h3 {
      margin-top: 0;
      color: #333;
    }
    .transcription-box {
      background: white;
      border: 2px solid #007bff;
      border-radius: 8px;
      padding: 1rem;
      margin: 1rem 0;
      min-height: 100px;
      font-size: 1.1em;
      line-height: 1.6;
    }
    .transcription-box.empty {
      color: #999;
      border-color: #ddd;
      font-style: italic;
    }
    .user-speech {
      color: #0066cc;
      font-weight: bold;
    }
    .system-msg {
      color: #666;
    }
    .connection-status {
      display: inline-block;
      padding: 0.25rem 0.5rem;
      border-radius: 4px;
      font-size: 0.8em;
      margin-left: 1rem;
    }
    .status-disconnected {
      background: #f8d7da;
      color: #721c24;
    }
    .status-connected {
      background: #d4edda;
      color: #155724;
    }
    .status-connecting {
      background: #fff3cd;
      color: #856404;
    }
  </style>
</head>
<body>
  <nav style="margin-bottom: 1rem;">
    <a href="../" style="color: #007bff; text-decoration: none; font-size: 0.9rem;">&larr; Comedy Colliders</a>
  </nav>
  <h1>Voice-to-Text Test <span id="connectionStatus" class="connection-status status-disconnected">Disconnected</span></h1>
  
  <div class="info">
    <strong>Purpose:</strong> Test speech-to-text using ElevenLabs agentic API in isolation:
    <br>• Connect to ElevenLabs WebSocket (no agent response)
    <br>• Capture audio → send to API → receive transcription
    <br>• Test conversation break detection and VAD (Voice Activity Detection)
  </div>

  <div class="api-section">
    <h3>ElevenLabs Configuration</h3>
    <label>API Key:</label>
    <div class="flex">
      <input type="password" id="apiKey" placeholder="Enter ElevenLabs API key">
      <button id="clear">Clear</button>
    </div>
    
    <label>Agent ID:</label>
    <input type="text" id="agentId" placeholder="Enter Agent ID">
  </div>

  <div class="transcription-box empty" id="transcriptionBox">
    Transcribed speech will appear here...
  </div>

  <button id="micButton" disabled>🎤 Start Listening</button>

  <div id="log"></div>

  <script>
    const $ = id => document.getElementById(id);
    const apiKey = $('apiKey'), agentId = $('agentId'), micButton = $('micButton'), log = $('log');
    const transcriptionBox = $('transcriptionBox'), connectionStatus = $('connectionStatus');
    
    // Global state
    let ws = null;
    let audioCtx = null;
    let stream = null;
    let worklet = null;
    let isListening = false;
    let conversationId = null;

    function addLog(msg, className = 'system-msg') {
      const lines = log.innerHTML.split('<br>').filter(Boolean);
      const timestamp = `[${new Date().toLocaleTimeString()}]`;
      let msgClass = className;
      
      if (msg.startsWith('USER:')) {
        msgClass = 'user-speech';
      }
      
      lines.push(`<span class="${msgClass}">${timestamp} ${msg}</span>`);
      if (lines.length > 100) lines.splice(0, lines.length - 100);
      log.innerHTML = lines.join('<br>');
      log.scrollTop = log.scrollHeight;
    }

    function updateConnectionStatus(status) {
      connectionStatus.className = `connection-status status-${status}`;
      connectionStatus.textContent = status.charAt(0).toUpperCase() + status.slice(1);
    }

    function updateButtons() {
      const hasConfig = apiKey.value && agentId.value; // Don't require voice - agent has its own
      const isConnected = ws && ws.readyState === WebSocket.OPEN;
      
      micButton.disabled = !hasConfig;
    }



    function openWebSocket() {
      if (ws && ws.readyState === WebSocket.OPEN) return;
      
      updateConnectionStatus('connecting');
      addLog('🔗 Connecting to ElevenLabs WebSocket...');
      
      const url = `wss://api.elevenlabs.io/v1/convai/conversation?agent_id=${agentId.value}`;
      ws = new WebSocket(url);
      ws.binaryType = "arraybuffer";
      
      ws.onopen = async () => {
        updateConnectionStatus('connected');
        addLog('✓ WebSocket connected');
        
        // Create AudioContext for capture
        audioCtx = new (AudioContext || webkitAudioContext)({ sampleRate: 16000 });
        addLog(`🎧 AudioContext created (sampleRate: ${audioCtx.sampleRate}Hz, state: ${audioCtx.state})`);
        
        // Resume if suspended (iOS requirement)
        if (audioCtx.state === 'suspended') {
          try {
            await audioCtx.resume();
            addLog(`🔄 AudioContext resumed, new state: ${audioCtx.state}`);
          } catch (e) {
            addLog(`⚠️ AudioContext resume failed: ${e.message}`);
          }
        }
        
        // Send initialization packet (required for ElevenLabs API)
        sendInitPacket();
        updateButtons();
      };
      
      ws.onmessage = handleServerEvent;
      
      ws.onclose = (event) => {
        updateConnectionStatus('disconnected');
        addLog(`🔌 WebSocket closed - Code: ${event.code}, Reason: "${event.reason}" (Clean: ${event.wasClean})`);
        
        // Common WebSocket close codes for reference
        const closeCodes = {
          1000: 'Normal closure',
          1001: 'Going away',
          1002: 'Protocol error',
          1003: 'Unsupported data',
          1005: 'No status code',
          1006: 'Abnormal closure',
          1007: 'Invalid data',
          1008: 'Policy violation',
          1009: 'Message too big',
          1010: 'Extension expected',
          1011: 'Server error',
          1012: 'Service restart',
          1013: 'Try again later',
          1014: 'Bad gateway',
          1015: 'TLS handshake failure'
        };
        
        const codeDescription = closeCodes[event.code] || 'Unknown';
        addLog(`📋 Close code ${event.code} means: ${codeDescription}`);
        
        cleanup();
        updateButtons();
      };
      
      ws.onerror = (error) => {
        updateConnectionStatus('disconnected');
        addLog('❌ WebSocket connection error');
        console.error('WebSocket error:', error);
      };
    }

    function sendInitPacket() {
      const init = {
        type: "conversation_initiation_client_data",
        conversation_initiation_client_data: {
          client_supplied_trace_id: crypto.randomUUID(),
          xi_api_key: apiKey.value // Add API key to init packet
        }
      };
      ws.send(JSON.stringify(init));
      addLog("🪄 Session initialization sent with API key");
    }

    async function handleServerEvent({ data }) {
      const msg = JSON.parse(data);
      
      // Debug logging - show ALL events for now
      if (msg.type !== "ping") {
        console.debug("📨 Received:", msg.type, msg);
        addLog(`📨 Server event: ${msg.type}`);
      }

      switch (msg.type) {
        case "ping":
          // Respond to ping to keep connection alive
          ws.send(JSON.stringify({ type: "pong", event_id: msg.ping_event.event_id }));
          break;

        case "vad_score":
          // Voice Activity Detection score - show ALL scores for debugging
          const vadScore = msg.vad_score_event.vad_score;
          if (!window._lastVadLog || Date.now() - window._lastVadLog > 1000) {
            addLog(`🎯 VAD score: ${vadScore.toFixed(3)} ${vadScore > 0.6 ? '(VOICE DETECTED!)' : '(quiet)'}`);
            window._lastVadLog = Date.now();
          }
          break;

        case "user_transcript":
          // This is what we're testing! Speech-to-text result
          const transcript = msg.user_transcript.text;
          addLog(`USER: ${transcript}`);
          
          // Display in transcription box
          if (transcriptionBox.classList.contains('empty')) {
            transcriptionBox.classList.remove('empty');
            transcriptionBox.textContent = '';
          }
          transcriptionBox.textContent += transcript + ' ';
          
          // Note: We're NOT sending this to an LLM - we just want the transcription
          break;

        case "audio":
          // We might receive audio back, but we don't want to play it for this test
          addLog(`🔊 Received audio chunk (ignoring for this test)`);
          break;

        case "conversation_initiation_client_data_response":
          addLog("✓ Conversation initialization acknowledged");
          break;

        case "conversation_initiation_metadata":
          conversationId = msg.conversation_initiation_metadata_event.conversation_id;
          addLog(`✓ Conversation metadata received (ID: ${conversationId.slice(-8)})`);
          break;

        default:
          console.warn("Unknown event:", msg.type, msg);
          addLog(`❓ UNKNOWN EVENT: ${msg.type} - ${JSON.stringify(msg).slice(0, 200)}...`);
      }
    }

    async function startListening() {
      try {
        addLog('🎤 Starting voice capture...');
        
        // Request microphone access
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
          }
        });
        
        const audioTracks = stream.getAudioTracks();
        addLog(`✓ Got ${audioTracks.length} audio track(s)`);
        
        // Create audio processing pipeline
        const source = audioCtx.createMediaStreamSource(stream);
        
        // Load and create worklet
        await audioCtx.audioWorklet.addModule('pcm-worklet.js');
        worklet = new AudioWorkletNode(audioCtx, 'pcm-encoder');
        
        // Handle audio data from worklet
        worklet.port.onmessage = ({ data }) => {
          // Handle debug messages from worklet
          if (data && typeof data === 'object' && data.type === 'debug') {
            addLog(`🔧 Worklet: ${data.message}`);
            return;
          }
          // Only process audio buffers (ArrayBuffer), skip debug messages
          if (!data || !(data instanceof ArrayBuffer)) {
            return;
          }
          
          if (!ws || ws.readyState !== WebSocket.OPEN) {
            return;
          }

          // Debug: Check if we're getting real audio data
          const view = new Uint8Array(data);
          let nonZeroCount = 0;
          let maxValue = 0;
          for (let i = 0; i < Math.min(100, view.length); i++) {
            if (view[i] !== 0) nonZeroCount++;
            maxValue = Math.max(maxValue, Math.abs(view[i]));
          }
          
          if (!window._audioDataDebug || Date.now() - window._audioDataDebug > 3000) {
            addLog(`🔍 Audio data check: ${nonZeroCount}/100 non-zero samples, max: ${maxValue}`);
            window._audioDataDebug = Date.now();
          }
          
          // If all zeros, don't send (might be silence or mic issue)
          if (nonZeroCount === 0) {
            if (!window._silenceWarned) {
              addLog('⚠️ WARNING: Audio data is all zeros - check microphone!');
              window._silenceWarned = true;
            }
            return; // Skip sending silent chunks
          }

          // Convert audio data to base64 for ElevenLabs API
          let b64;
          if (typeof data === "string") {
            b64 = data;
          } else {
            let bin = "";
            for (let i = 0; i < view.length; i++) bin += String.fromCharCode(view[i]);
            b64 = btoa(bin);
          }

          // Send audio chunk to ElevenLabs - try different format
          const audioMessage = {
            type: "user_audio_chunk",
            user_audio_chunk_event: {  // Maybe it needs _event suffix?
              audio_base_64: b64,      // Maybe it needs _base_64 suffix?
              timestamp: Date.now()
            }
          };
          
          ws.send(JSON.stringify(audioMessage));
          
          // Also try without timestamp
          // ws.send(JSON.stringify({
          //   type: "user_audio_chunk", 
          //   user_audio_chunk: { audio: b64 }
          // }));
          
          // Debug the exact message we're sending
          if (!window._debuggedFirstChunk) {
            window._debuggedFirstChunk = true;
            addLog(`📝 First real audio message format test`);
          }

          // Occasional logging
          if (!window._lastAudioLog || Date.now() - window._lastAudioLog > 2000) {
            addLog(`🎧 Sending audio chunks (${b64.length} bytes base64)`);
            window._lastAudioLog = Date.now();
          }
        };
        
        // Connect audio pipeline
        source.connect(worklet);
        addLog('🔗 Audio pipeline connected: Mic → Worklet → WebSocket');
        
        isListening = true;
        micButton.classList.add('active');
        micButton.textContent = "🔴 Stop Listening";
        
      } catch (error) {
        addLog(`❌ Failed to start listening: ${error.message}`);
        console.error('Start listening error:', error);
      }
    }

    function stopListening() {
      addLog('⏹️ Stopping voice capture...');
      
      if (worklet) {
        worklet.port.onmessage = null;
        worklet.disconnect();
        worklet = null;
      }
      
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
        stream = null;
      }
      
      isListening = false;
      micButton.classList.remove('active');
      micButton.textContent = "🎤 Start Listening";
      
      addLog('🛑 Voice capture stopped');
    }

    function cleanup() {
      if (isListening) {
        stopListening();
      }
      
      if (ws) {
        ws.close();
        ws = null;
      }
      
      conversationId = null;
    }

    // Event listeners
    apiKey.oninput = () => {
      if (apiKey.value.length > 20) {
        localStorage.elevenlabs_key = apiKey.value;
        addLog(`🔑 API key entered (${apiKey.value.slice(0,4)}...${apiKey.value.slice(-4)})`);
        updateButtons();
      } else {
        updateButtons();
      }
    };

    agentId.oninput = updateButtons;

    $('clear').onclick = () => {
      delete localStorage.elevenlabs_key;
      apiKey.value = '';
      addLog('🗑️ API key cleared');
      updateButtons();
    };

    micButton.onclick = () => {
      if (!isListening) {
        // Start listening
        if (!ws || ws.readyState !== WebSocket.OPEN) {
          openWebSocket();
          // Wait a moment for connection to establish
          setTimeout(() => {
            if (ws && ws.readyState === WebSocket.OPEN) {
              startListening();
            } else {
              addLog("❌ Connection not ready - try again");
            }
          }, 1000);
        } else {
          startListening();
        }
      } else {
        // Stop listening
        stopListening();
      }
    };

    // Initialize
    document.addEventListener('DOMContentLoaded', () => {
      addLog('Voice-to-Text Test initialized');
      updateButtons();
      
      // Load saved API key
      if (localStorage.elevenlabs_key) {
        apiKey.value = localStorage.elevenlabs_key;
        addLog(`📁 Loaded saved API key (${localStorage.elevenlabs_key.slice(0,4)}...${localStorage.elevenlabs_key.slice(-4)})`);
        updateButtons();
      }
      
      // iOS audio unlock
      document.addEventListener('touchstart', () => {
        addLog('📱 Touch detected - audio unlocked for iOS');
      }, { once: true });
    });

    // Cleanup on page unload
    window.addEventListener('beforeunload', cleanup);
  </script>
</body>
</html>